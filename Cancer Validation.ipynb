{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import cross_validation\n",
    "import math\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"breast_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'id number'], axis=1)\n",
    "y = data.pop(\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estimators = [300,400,500]\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "min_samples_split = [3,5,7]\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=1)\n",
    "estimator = GridSearchCV(rfc,\n",
    "                         dict(n_estimators=n_estimators,\n",
    "                              max_features=max_features,\n",
    "                              min_samples_split=min_samples_split\n",
    "                             ), cv=None, n_jobs=1, refit=True)\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwritten for Readability\n"
     ]
    }
   ],
   "source": [
    "#estimator.fit(X_train, y_train)\n",
    "print (\"Overwritten for Readability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Graham\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_rfc = estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.964285714286\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, best_rfc.predict(X_test))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.97        95\n",
      "          1       0.93      0.96      0.95        45\n",
      "\n",
      "avg / total       0.96      0.96      0.96       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, best_rfc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:  0.996257309942\n"
     ]
    }
   ],
   "source": [
    "roc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\n",
    "print (\"AUC Score: \", roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwritten for Readability\n"
     ]
    }
   ],
   "source": [
    "#scores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n",
    "print (\"Overwritten for Readability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.970080 +/-  0.018412\n",
      "95 percent probability that if this experiment were repeated over and over the average score would be between 0.951668 and 0.988492\n"
     ]
    }
   ],
   "source": [
    "mean_score = scores.mean()\n",
    "std_dev = scores.std()\n",
    "std_error = scores.std() / math.sqrt(scores.shape[0])\n",
    "ci =  2.262 * std_error\n",
    "lower_bound = mean_score - ci\n",
    "upper_bound = mean_score + ci\n",
    "\n",
    "print (\"Score is %f +/-  %f\" % (mean_score, ci))\n",
    "print ('95 percent probability that if this experiment were repeated over and over the average score would be between %f and %f' % (lower_bound, upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary\n",
    "\n",
    "The AUC of the model was 0.996 (using a holdout)\n",
    "\n",
    "The precision and recall are both 0.96\n",
    "\n",
    "The validation showed an overfitting of the data, as the range for the AUC should fall between 0.951668 and 0.988492 (with an average of 0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
